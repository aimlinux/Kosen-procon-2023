8.22 : 

AIについては強化学習に限られてくる？
pythonを使用してAIを対決させて強化学習をする

[例]： Q-Learning（Q値）
       DQN


/////////////////////////////////////////////////////////////////////////////////////////
        ---- 強化学習のセットアップ ----
        強化学習環境を作成
            OpenAI Gym とか Stable Baselinesを使う

        エージェントの設定
            各AIをエージェントとして設定して強化学習環境内で行動
            Q-learningやDeep Q Network（DQN）、Policy Gradient
            などのアルゴリズムを使用してエージェントを設定

        対決環境を設定する
            エージェント同士を対決させる
            この環境はゲームやシミュレーションなどの形で表現される
            エージェントはこの環境内で行動を選択し、報酬を得て学習を進める

        エピソードの実行（エージェント同士を対決）
            複数のエピソードを実行してエージェント同士を対決させる
            ⇒環境内で行動と学習が行われる

        エージェントの性能を評価する
            エピソードを実行した後、エージェントの性能を評価する
            報酬の合計などを基にエージェントの性能を比較し、学習のパラメータ調整を行う
            学習率や報酬の設定などを調整することで、エージェントの性能向上を図る

        これらを繰り返してエージェントの性能を向上させる
//////////////////////////////////////////////////////////////////////////////////////



08/24 : 

初期段階のエージェントを作成するのはどうするか？
    ・単純なのは全ての行動を糖確率でランダムに行動させる。
    ・モンテカルロを使う方法もある？



8/28 : 

遅れてC25.pyも完成
これで全部



8/29 : 
次までにQ-learningとDQNについて調べておく


8/31 :
深層強化学習で行くことはほぼ決まりとして、
あとは報酬のつけ方を決めることが大事だと思う。

先生曰く、pythonで強化学習をするには時間がかかりすぎるためC++が良い
pythonだけど、anacondaとjupyterを使ったらとりあえずgymの銅か確認までは出来た。

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

        囲碁のような対戦ゲームをOpenAI Gym環境内で深層強化学習させるには、以下の手順に従って進めることができます。この例では、強化学習ライブラリのChainerRLを使用しますが、他のライブラリでも基本的な流れは同じです。

        環境の作成: Gym環境を作成し、囲碁の対戦ゲームをエージェントと対戦させるための環境を構築します。

        エージェントの作成: ChainerRLを使用して、強化学習エージェントを作成します。例えば、Deep Q-Network (DQN) などを使用することができます。

        訓練ループの構築: エージェントを訓練するためのループを構築します。これにはエピソードを実行し、エージェントの方策を更新するプロセスが含まれます。

        方策更新: エージェントの方策を更新するアルゴリズムを選び、適切な学習手法を適用します。例えば、DQNではExperience ReplayやTarget Networkを使用します。

        トレーニング: 訓練を実行し、エージェントが囲碁の対戦ゲームでどのように学習していくかを監視します。訓練中にエージェントが獲得する報酬を適切に設計することが重要です。

        以下に、これらのステップを詳しく説明します。

        1. 環境の作成
        Gym環境を作成し、囲碁の対戦ゲームを実装します。Gym環境は、gym.Envクラスを継承して作成します。reset()メソッドで初期化し、step()メソッドでエージェントの行動を実行し、新しい状態と報酬を返します。

        2. エージェントの作成
        ChainerRLを使用してエージェントを作成します。chainerrl.agentsモジュールには様々なエージェントが実装されています。例えば、DQNやDoubleDQNなどがあります。エージェントの初期化時に、行動空間や状態空間の次元などを指定します。

        3. 訓練ループの構築
        訓練ループを構築し、エピソードごとにエージェントを対戦させて方策を更新します。エピソードの中で、状態の観測、エージェントの行動選択、環境への行動の実行、新しい状態と報酬の受け取りなどが行われます。

        4. 方策更新
        エージェントの方策を更新するアルゴリズムを選びます。DQNの場合、Experience ReplayやTarget Networkを使用して安定した学習を行います。

        5. トレーニング
        上記のステップを組み合わせてエージェントをトレーニングします。トレーニング中には、報酬の設計や学習率などのハイパーパラメータの調整が重要です。

        以上の手順を実行することで、Gym環境内で囲碁の対戦ゲームを深層強化学習させることができます。ただし、囲碁は非常に複雑なゲームであり、DQNなどの基本的なアルゴリズムでは十分な性能が得られない可能性があります。より高度なアルゴリズムやアプローチを検討することも大切です。

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////